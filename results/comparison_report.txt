
================================================================================
          KNOWLEDGE DISTILLATION COMPARATIVE ANALYSIS REPORT
================================================================================

EXPERIMENT CONFIGURATION
------------------------
Dataset: CIFAR100
Teacher Model: efficientnet_b2
Student Model: efficientnet_b0
Distillation Temperature: 4.0
Alpha (soft target weight): 0.7
Training Epochs: 50
Batch Size: 16
Learning Rate: 0.001

================================================================================
MODEL SPECIFICATIONS
================================================================================

                          Baseline B0          Distilled B0
                          -----------          ------------
Parameters:                  4,135,648         4,135,648
Model Size (MB):                 15.94             15.94
Inference Time (ms):             11.22             12.00

================================================================================
PERFORMANCE COMPARISON
================================================================================

                          Baseline B0          Distilled B0         Improvement
                          -----------          ------------         -----------
Top-1 Accuracy (%):              57.72             62.97        +5.25
Top-5 Accuracy (%):              84.84             82.15        -2.69
F1 Score Macro (%):              57.67             63.16        +5.49
F1 Score Weighted (%):           57.67             63.16

================================================================================
KEY FINDINGS
================================================================================

1. ACCURACY IMPROVEMENT:
   - Knowledge distillation improved Top-1 accuracy by 5.25%
   - Top-5 accuracy decreased by 2.69%

2. MODEL EFFICIENCY:
   - Both models have identical architecture (EfficientNet-B0)
   - Same inference time and model size
   - Distillation provides accuracy gains with no additional inference cost

3. KNOWLEDGE TRANSFER:
   - Teacher model (efficientnet_b2) knowledge successfully transferred
   - Student model learns from soft probability distributions
   - Temperature T=4.0 used for softening predictions

================================================================================
CONCLUSIONS
================================================================================

[OK] Knowledge distillation successfully improved model performance
[OK] Distilled model shows better generalization

The experiment demonstrates that knowledge distillation can transfer knowledge 
from a larger teacher model to a smaller student model, potentially achieving 
better performance than training the student model from scratch.

================================================================================
