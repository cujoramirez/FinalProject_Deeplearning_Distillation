{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2db915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading TinyImageNet (~240MB)...\n",
      "Extracting TinyImageNet...\n",
      "Extracting TinyImageNet...\n",
      "Done extracting.\n",
      "Using device: cuda\n",
      "Done extracting.\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training teacher EfficientNet-B2 on TinyImageNet...\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35.2M/35.2M [00:00<00:00, 166MB/s]\n",
      "\n",
      "Teacher-B2 Train E1/25: 100%|██████████| 3125/3125 [03:47<00:00, 13.72it/s, loss=1.69]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher-B2 Epoch 1: train_acc=33.09% val_acc=46.67%\n",
      "  Saved new best Teacher-B2 to ./checkpoints_aktp/teacher_b2_tiny.pth (val_acc=46.67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher-B2 Train E2/25: 100%|██████████| 3125/3125 [03:44<00:00, 13.91it/s, loss=1.92]\n",
      "Teacher-B2 Train E2/25: 100%|██████████| 3125/3125 [03:44<00:00, 13.91it/s, loss=1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher-B2 Epoch 2: train_acc=47.38% val_acc=52.57%\n",
      "  Saved new best Teacher-B2 to ./checkpoints_aktp/teacher_b2_tiny.pth (val_acc=52.57%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher-B2 Train E3/25: 100%|██████████| 3125/3125 [03:46<00:00, 13.77it/s, loss=2.44] \n",
      "Teacher-B2 Train E3/25: 100%|██████████| 3125/3125 [03:46<00:00, 13.77it/s, loss=2.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher-B2 Epoch 3: train_acc=52.31% val_acc=53.30%\n",
      "  Saved new best Teacher-B2 to ./checkpoints_aktp/teacher_b2_tiny.pth (val_acc=53.30%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher-B2 Train E4/25: 100%|██████████| 3125/3125 [03:47<00:00, 13.72it/s, loss=2.06] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher-B2 Epoch 4: train_acc=56.02% val_acc=54.83%\n",
      "  Saved new best Teacher-B2 to ./checkpoints_aktp/teacher_b2_tiny.pth (val_acc=54.83%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher-B2 Train E5/25: 100%|██████████| 3125/3125 [03:44<00:00, 13.89it/s, loss=1.33] \n",
      "Teacher-B2 Train E5/25: 100%|██████████| 3125/3125 [03:44<00:00, 13.89it/s, loss=1.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher-B2 Epoch 5: train_acc=59.29% val_acc=55.74%\n",
      "  Saved new best Teacher-B2 to ./checkpoints_aktp/teacher_b2_tiny.pth (val_acc=55.74%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher-B2 Train E6/25: 100%|██████████| 3125/3125 [03:46<00:00, 13.82it/s, loss=1.59] \n",
      "Teacher-B2 Train E6/25: 100%|██████████| 3125/3125 [03:46<00:00, 13.82it/s, loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher-B2 Epoch 6: train_acc=62.21% val_acc=56.75%\n",
      "  Saved new best Teacher-B2 to ./checkpoints_aktp/teacher_b2_tiny.pth (val_acc=56.75%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher-B2 Train E7/25: 100%|██████████| 3125/3125 [03:48<00:00, 13.66it/s, loss=0.939]\n",
      "Teacher-B2 Train E7/25: 100%|██████████| 3125/3125 [03:48<00:00, 13.66it/s, loss=0.939]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import (\n",
    "    EfficientNet_B0_Weights,\n",
    "    EfficientNet_B2_Weights,\n",
    "    ResNet18_Weights,\n",
    " )\n",
    "from tqdm import tqdm\n",
    "class TinyImageNetValDataset(Dataset):\n",
    "    \"\"\"TinyImageNet validation dataset using val_annotations.txt labels.\"\"\"\n",
    "\n",
    "    def __init__(self, root: str, transform=None, class_to_idx=None):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        annotations = os.path.join(root, \"val_annotations.txt\")\n",
    "        images_dir = os.path.join(root, \"images\")\n",
    "\n",
    "        self.samples = []\n",
    "        # Ensure class indices align with train set if provided\n",
    "        self.class_to_idx = class_to_idx if class_to_idx is not None else {}\n",
    "\n",
    "        with open(annotations, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                img, cls = parts[0], parts[1]\n",
    "                if cls not in self.class_to_idx:\n",
    "                    self.class_to_idx[cls] = len(self.class_to_idx)\n",
    "                self.samples.append((os.path.join(images_dir, img), self.class_to_idx[cls]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.samples[idx]\n",
    "        with open(path, \"rb\") as f:\n",
    "            img = Image.open(f).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Paths\n",
    "    data_path: str = \"./data\"\n",
    "    checkpoints_dir: str = \"./checkpoints_aktp\"\n",
    "    logs_dir: str = \"./results_aktp\"\n",
    "\n",
    "    # Dataset\n",
    "    dataset_name: str = \"TinyImageNet\"\n",
    "    num_classes: int = 200\n",
    "    image_size: int = 64\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 4\n",
    "\n",
    "    # Teacher fine-tuning\n",
    "    teacher_epochs: int = 25\n",
    "    teacher_lr: float = 5e-4\n",
    "    teacher_weight_decay: float = 1e-4\n",
    "    teacher_early_stop: int = 5\n",
    "    teacher_b2_ckpt: str = \"./checkpoints_aktp/teacher_b2_tiny.pth\"\n",
    "    teacher_r18_ckpt: str = \"./checkpoints_aktp/teacher_r18_tiny.pth\"\n",
    "    train_teachers_if_missing: bool = True\n",
    "\n",
    "    # Student pretrain\n",
    "    student_pretrain_epochs: int = 0  # not used when distilling from scratch\n",
    "    student_pretrain_lr: float = 5e-4\n",
    "    student_pretrain_ckpt: str = \"./checkpoints_aktp/student_b0_tiny_pretrain.pth\"  # optional load\n",
    "    pretrain_student_if_missing: bool = False  # keep False to distill from ImageNet init\n",
    "\n",
    "    # Distillation\n",
    "    distill_epochs: int = 50\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    early_stopping_patience: int = 10\n",
    "    temperature: float = 4.0\n",
    "    gamma_cal: float = 0.5\n",
    "\n",
    "    # Device\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def serialize_cfg(cfg):\n",
    "    return {k: (str(v) if isinstance(v, torch.device) else v) for k, v in cfg.__dict__.items()}\n",
    "\n",
    "def ensure_tiny_imagenet(cfg: Config):\n",
    "    data_root = os.path.join(cfg.data_path, \"tiny-imagenet-200\")\n",
    "    if os.path.isdir(data_root):\n",
    "        print(f\"TinyImageNet found at {data_root}\")\n",
    "        return data_root\n",
    "    os.makedirs(cfg.data_path, exist_ok=True)\n",
    "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "    zip_path = os.path.join(cfg.data_path, \"tiny-imagenet-200.zip\")\n",
    "    if not os.path.isfile(zip_path):\n",
    "        print(\"Downloading TinyImageNet (~240MB)...\")\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "    print(\"Extracting TinyImageNet...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(cfg.data_path)\n",
    "    print(\"Done extracting.\")\n",
    "    return data_root\n",
    "\n",
    "# --- 1. Interchangeable Dataset Wrapper ---\n",
    "def get_tinyimagenet_loaders(config: Config):\n",
    "    \"\"\"TinyImageNet train/val loaders using official train/val split.\"\"\"\n",
    "    data_root = os.path.join(config.data_path, \"tiny-imagenet-200\")\n",
    "    train_dir = os.path.join(data_root, \"train\")\n",
    "    val_dir = os.path.join(data_root, \"val\")\n",
    "\n",
    "    if not os.path.isdir(data_root):\n",
    "        raise FileNotFoundError(\n",
    "            f\"TinyImageNet not found at {data_root}. Download and extract to this path.\"\n",
    "        )\n",
    "\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(config.image_size, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize(config.image_size),\n",
    "        transforms.CenterCrop(config.image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    train_set = datasets.ImageFolder(train_dir, transform=train_tf)\n",
    "    val_set = TinyImageNetValDataset(val_dir, transform=val_tf, class_to_idx=train_set.class_to_idx)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def split_train_val(train_loader, val_ratio=0.1):\n",
    "    \"\"\"Utility to create an explicit val split from train if desired.\"\"\"\n",
    "    dataset = train_loader.dataset\n",
    "    val_size = int(len(dataset) * val_ratio)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_subset, val_subset = random_split(dataset, [train_size, val_size])\n",
    "    return train_subset, val_subset\n",
    "\n",
    "# --- 2. The Combiner Module (Logit Fusion) ---\n",
    "class CombinerNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses logits from multiple teachers into a single soft target.\n",
    "    Reference CALM Paper Stage 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_teachers, num_classes, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        input_dim = num_teachers * num_classes\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, logits_list):\n",
    "        # Concatenate logits: [Batch, Class] + [Batch, Class] -> [Batch, Class*2]\n",
    "        combined = torch.cat(logits_list, dim=1)\n",
    "        return self.net(combined)\n",
    "\n",
    "# --- 3. AKTP Weighting Module ---\n",
    "class AKTP(nn.Module):\n",
    "    \"\"\"\n",
    "    Adaptive Knowledge Transfer Protocol.\n",
    "    Calculates lambda based on Student Entropy and Teacher Disagreement.\n",
    "    Reference CALM Paper[cite: 219, 237].\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input: 2 dims (Entropy, Disagreement) -> Output: 1 scalar (Lambda)\n",
    "        self.fc = nn.Linear(2, 1)\n",
    "        # Initialize bias to negative to prefer distillation (lambda close to 0) initially\n",
    "        nn.init.constant_(self.fc.bias, -1.0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits_1, teacher_logits_2):\n",
    "        # 1. Calculate Student Entropy H(S(x))\n",
    "        probs_student = F.softmax(student_logits, dim=1)\n",
    "        log_probs_student = F.log_softmax(student_logits, dim=1)\n",
    "        entropy = -torch.sum(probs_student * log_probs_student, dim=1, keepdim=True) # [Batch, 1]\n",
    "\n",
    "        # 2. Calculate Teacher Disagreement D(T1, T2) using symmetric KL\n",
    "        # Note: Paper uses disagreement between students, we adapt to disagreement between Teachers\n",
    "        log_prob_t1 = F.log_softmax(teacher_logits_1, dim=1)\n",
    "        prob_t2 = F.softmax(teacher_logits_2, dim=1)\n",
    "        \n",
    "        log_prob_t2 = F.log_softmax(teacher_logits_2, dim=1)\n",
    "        prob_t1 = F.softmax(teacher_logits_1, dim=1)\n",
    "        \n",
    "        kl1 = F.kl_div(log_prob_t1, prob_t2, reduction='none', log_target=False).sum(1, keepdim=True)\n",
    "        kl2 = F.kl_div(log_prob_t2, prob_t1, reduction='none', log_target=False).sum(1, keepdim=True)\n",
    "        disagreement = 0.5 * (kl1 + kl2) # [Batch, 1]\n",
    "\n",
    "        # 3. Compute Lambda\n",
    "        # Normalize inputs roughly for stability\n",
    "        features = torch.cat([entropy, disagreement], dim=1)\n",
    "        return self.sigmoid(self.fc(features)) # Returns lambda per sample [Batch, 1]\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Stop training if validation metric does not improve after patience epochs.\"\"\"\n",
    "\n",
    "    def __init__(self, patience: int = 10, min_delta: float = 0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = None\n",
    "        self.count = 0\n",
    "\n",
    "    def step(self, metric: float) -> bool:\n",
    "        if self.best is None or metric > self.best + self.min_delta:\n",
    "            self.best = metric\n",
    "            self.count = 0\n",
    "            return False\n",
    "        self.count += 1\n",
    "        return self.count >= self.patience\n",
    "\n",
    "def build_effnet_b2(num_classes: int):\n",
    "    model = models.efficientnet_b2(weights=EfficientNet_B2_Weights.IMAGENET1K_V1)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_resnet18(num_classes: int):\n",
    "    model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_effnet_b0(num_classes: int):\n",
    "    # Student starts from scratch (no ImageNet weights) as requested\n",
    "    model = models.efficientnet_b0(weights=None)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_classifier(model: nn.Module, train_loader, val_loader, config: Config, epochs: int, lr: float, weight_decay: float, patience: int, device: torch.device, tag: str, save_path: str):\n",
    "    \"\"\"Standard CE training loop with early stopping; saves best checkpoint.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    stopper = EarlyStopping(patience=patience)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"{tag} Train E{epoch+1}/{epochs}\")\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                logits = model(images)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "        print(f\"{tag} Epoch {epoch+1}: train_acc={train_acc:.2f}% val_acc={val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"  Saved new best {tag} to {save_path} (val_acc={val_acc:.2f}%)\")\n",
    "            stop_now = False\n",
    "        else:\n",
    "            stop_now = stopper.step(val_acc)\n",
    "        if stop_now:\n",
    "            print(f\"Early stopping {tag} at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    return save_path\n",
    "\n",
    "\n",
    "def load_or_train_teachers(train_loader, val_loader, cfg: Config):\n",
    "    \"\"\"Ensure TinyImageNet-finetuned teachers are available.\"\"\"\n",
    "    b2_path = cfg.teacher_b2_ckpt\n",
    "    r18_path = cfg.teacher_r18_ckpt\n",
    "    os.makedirs(cfg.checkpoints_dir, exist_ok=True)\n",
    "\n",
    "    need_b2 = not os.path.isfile(b2_path)\n",
    "    need_r18 = not os.path.isfile(r18_path)\n",
    "\n",
    "    if (need_b2 or need_r18) and not cfg.train_teachers_if_missing:\n",
    "        missing = [\"B2\" if need_b2 else None, \"R18\" if need_r18 else None]\n",
    "        missing = [m for m in missing if m]\n",
    "        raise FileNotFoundError(f\"Missing teacher checkpoints: {missing}. Enable training or provide paths.\")\n",
    "\n",
    "    if need_b2:\n",
    "        print(\"Training teacher EfficientNet-B2 on TinyImageNet...\")\n",
    "        model = build_effnet_b2(cfg.num_classes)\n",
    "        train_classifier(model, train_loader, val_loader, cfg, cfg.teacher_epochs, cfg.teacher_lr, cfg.teacher_weight_decay, cfg.teacher_early_stop, cfg.device, \"Teacher-B2\", b2_path)\n",
    "    if need_r18:\n",
    "        print(\"Training teacher ResNet18 on TinyImageNet...\")\n",
    "        model = build_resnet18(cfg.num_classes)\n",
    "        train_classifier(model, train_loader, val_loader, cfg, cfg.teacher_epochs, cfg.teacher_lr, cfg.teacher_weight_decay, cfg.teacher_early_stop, cfg.device, \"Teacher-R18\", r18_path)\n",
    "\n",
    "    # Load teachers\n",
    "    b2 = build_effnet_b2(cfg.num_classes)\n",
    "    b2.load_state_dict(torch.load(b2_path, map_location=cfg.device))\n",
    "    b2.to(cfg.device)\n",
    "    b2.eval()\n",
    "    for p in b2.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    r18 = build_resnet18(cfg.num_classes)\n",
    "    r18.load_state_dict(torch.load(r18_path, map_location=cfg.device))\n",
    "    r18.to(cfg.device)\n",
    "    r18.eval()\n",
    "    for p in r18.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    return b2, r18\n",
    "\n",
    "\n",
    "def load_or_pretrain_student(train_loader, val_loader, cfg: Config):\n",
    "    os.makedirs(cfg.checkpoints_dir, exist_ok=True)\n",
    "    path = cfg.student_pretrain_ckpt\n",
    "\n",
    "    # If a checkpoint exists, load it; otherwise start from scratch (no ImageNet-1k weights).\n",
    "    if os.path.isfile(path):\n",
    "        print(f\"Loading existing student checkpoint from {path}\")\n",
    "        model = build_effnet_b0(cfg.num_classes)\n",
    "        model.load_state_dict(torch.load(path, map_location=cfg.device))\n",
    "    else:\n",
    "        print(\"No student checkpoint found; starting student from scratch (no ImageNet-1k weights) and distilling with AKTP.\")\n",
    "        model = build_effnet_b0(cfg.num_classes)\n",
    "\n",
    "    model.to(cfg.device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def distill_with_aktp(train_loader, val_loader, teachers, student, cfg: Config):\n",
    "    t1, t2 = teachers\n",
    "    combiner = CombinerNetwork(num_teachers=2, num_classes=cfg.num_classes).to(cfg.device)\n",
    "    aktp_module = AKTP().to(cfg.device)\n",
    "\n",
    "    optimizer = optim.AdamW([\n",
    "        {\"params\": student.parameters(), \"lr\": cfg.lr},\n",
    "        {\"params\": combiner.parameters(), \"lr\": cfg.lr},\n",
    "        {\"params\": aktp_module.parameters(), \"lr\": cfg.lr},\n",
    "    ], weight_decay=cfg.weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.distill_epochs)\n",
    "    stopper = EarlyStopping(patience=cfg.early_stopping_patience)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    os.makedirs(cfg.checkpoints_dir, exist_ok=True)\n",
    "    best_path = os.path.join(cfg.checkpoints_dir, \"b0_aktp_tiny_best.pth\")\n",
    "\n",
    "    for epoch in range(cfg.distill_epochs):\n",
    "        student.train(); combiner.train(); aktp_module.train()\n",
    "        total_loss = 0.0\n",
    "        avg_lambda = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"AKTP Distill E{epoch+1}/{cfg.distill_epochs}\")\n",
    "        for inputs, targets in pbar:\n",
    "            inputs, targets = inputs.to(cfg.device), targets.to(cfg.device)\n",
    "            with torch.no_grad():\n",
    "                l_t1 = t1(inputs)\n",
    "                l_t2 = t2(inputs)\n",
    "            fused_logits = combiner([l_t1, l_t2])\n",
    "            p_comb = F.softmax(fused_logits / cfg.temperature, dim=1)\n",
    "\n",
    "            l_student = student(inputs)\n",
    "            lambda_val = aktp_module(l_student, l_t1, l_t2)\n",
    "\n",
    "            ce_loss = F.cross_entropy(l_student, targets, reduction=\"none\")\n",
    "            log_prob_student = F.log_softmax(l_student / cfg.temperature, dim=1)\n",
    "            # Standard KD scaling multiplies by T^2 to keep gradient magnitudes stable\n",
    "            kd_loss = F.kl_div(log_prob_student, p_comb, reduction=\"none\").sum(dim=1) * (cfg.temperature ** 2)\n",
    "            conf, pred = torch.max(F.softmax(l_student, dim=1), 1)\n",
    "            acc = (pred == targets).float()\n",
    "            cal_loss = (conf - acc) ** 2\n",
    "\n",
    "            final_loss = (lambda_val.squeeze() * ce_loss) + ((1 - lambda_val.squeeze()) * kd_loss) + (cfg.gamma_cal * cal_loss)\n",
    "            final_loss = final_loss.mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            final_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += final_loss.item()\n",
    "            avg_lambda += lambda_val.mean().item()\n",
    "            pbar.set_postfix({\"loss\": final_loss.item(), \"mean_lambda\": lambda_val.mean().item()})\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        student.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(cfg.device), targets.to(cfg.device)\n",
    "                outputs = student(inputs)\n",
    "                pred = outputs.argmax(dim=1)\n",
    "                correct += (pred == targets).sum().item()\n",
    "                total += targets.size(0)\n",
    "        acc = 100.0 * correct / total\n",
    "        mean_lambda = avg_lambda / max(len(train_loader), 1)\n",
    "        print(f\"Epoch {epoch+1}: val_acc={acc:.2f}% mean_lambda={mean_lambda:.4f}\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(student.state_dict(), best_path)\n",
    "            print(f\"  Saved best distilled student at {best_path} (acc={acc:.2f}%)\")\n",
    "            stop_now = False\n",
    "        else:\n",
    "            stop_now = stopper.step(acc)\n",
    "        if stop_now:\n",
    "            print(f\"Early stopping distillation at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    latest_path = os.path.join(cfg.checkpoints_dir, \"b0_aktp_tiny_latest.pth\")\n",
    "    torch.save(student.state_dict(), latest_path)\n",
    "    return best_path, latest_path, best_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "    cfg = Config()\n",
    "    os.makedirs(cfg.checkpoints_dir, exist_ok=True)\n",
    "    os.makedirs(cfg.logs_dir, exist_ok=True)\n",
    "    os.makedirs(cfg.data_path, exist_ok=True)\n",
    "\n",
    "    # Save config snapshot\n",
    "    with open(os.path.join(cfg.logs_dir, \"aktp_tiny_config.json\"), \"w\") as f:\n",
    "        json.dump(serialize_cfg(cfg), f, indent=2)\n",
    "\n",
    "    ensure_tiny_imagenet(cfg)\n",
    "\n",
    "    print(f\"Using device: {cfg.device}\")\n",
    "    train_loader, val_loader = get_tinyimagenet_loaders(cfg)\n",
    "\n",
    "    # Stage 1: Teachers\n",
    "    t1, t2 = load_or_train_teachers(train_loader, val_loader, cfg)\n",
    "    print(\"Teachers ready (TinyImageNet-finetuned).\")\n",
    "\n",
    "    # Stage 2: Student pretrain (optional)\n",
    "    student = load_or_pretrain_student(train_loader, val_loader, cfg)\n",
    "\n",
    "    # Stage 3: AKTP distillation\n",
    "    best_path, latest_path, best_acc = distill_with_aktp(train_loader, val_loader, (t1, t2), student, cfg)\n",
    "    print(f\"Distillation complete. Best val acc: {best_acc:.2f}%\")\n",
    "    print(f\"Best student checkpoint: {best_path}\")\n",
    "    print(f\"Latest student checkpoint: {latest_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
